#!/usr/bin/env python3

from urllib.parse import urljoin
import requests
from lxml import etree
import re
from bs4 import BeautifulSoup
'''Issues:
1. We end up with relative urls sometimes so I need to fix that
2. We need a better criteria for ending the crawling
3. We need to add timers to the crawling intervals
4. Throw in a bit of randomness into the websites to scrape
5. Throw in a bit of randomness with the waiting time
'''



urls=set()
visited=[]

def get_content(url):
    '''Opens content of browser'''
    try:
        response = requests.get(url).content
        print('got contents')
    except:
            print('invalid url: {0}'.format(url))
    return response

def stack_urls(new_urls):
    """Stacks the urls"""
    urls.update(new_urls)
    print('I have stacked')

def track(url):
    '''Keeps track of urls'''
    visited.append(url)
    print('tracking...')

def get_links(url):
    '''Get the links from the webpage'''
    #get contents
    contents=get_content(url)
    #build html tree
    tree= etree.HTML(contents)
    #grab the links
    links =(link.get('href') for link in tree.xpath('//a[@href]'))
    #stacks the links
    stack_urls([linkk for linkk in links if 'www' in linkk])
    print('got links')

def scrapper(baseurl):
    '''This is a crawler '''
    totalurl= 0
    urls.update([baseurl])

    #crawls the web
    while True:
        #new websites to craw
        oldurl=urls.pop()
        while True:
            #Checks if url hasn't been crawled already
            if not oldurl in visited:
                get_links(oldurl)
                break
            else:
                if len(urls)==0:
                    break
                else:
                    oldurl=urls.pop()
        track(oldurl)
        totalurl+=len(urls)   # keeps track of the size of stack
        print('total links {0}'.format(totalurl))
        #condition for ending the crawing
        if totalurl==0 or totalurl>=6000:
            return







if __name__  == '__main__':

    url= "http://nytimes.com"
    scrapper(url)
    print(urls)
    print('total websites visited: {0}'.format(len(visited)))


#response=requests.get(url).content

#parser=etree.HTMLParser(encoding='utf-8')

#tree=etree.fromstring(reponse,parser)

#print([link.get('href') for link in tree.xpath('//a[@href]')])

#soup=BeautifulSoup(response,'lxml')

#print([link.string for link in soup.find_all('a',href=True)])
